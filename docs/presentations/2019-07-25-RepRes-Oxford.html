<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Reproducible Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Maja Založnik" />
    <meta name="date" content="2019-07-25" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reproducible Research
## (in the Humanities)
### Maja Založnik
### 25 July 2019

---




# Outline of this talk

--

* ### Reproducibility: *What?*


???

So first of all we should try to get on the same playing field, I want to briefly talk about what Reproducibility is, what we mean by reproducible research. 
--

* ### Reproducibility: *Why?*

???

Next i will talk about WHY reproducible research is important, why it should be important to you

--

* ### Reproducibility: *Where?*


???

By where I mean "in the humanities", so I'll briefly touch on how we think about reproducibility in computational fields and in non-computational fields, although the focus of the talk is of course the former. 

---
# Outline of this talk

1. ### Reproducibility: *What?*


2. ### Reproducibility: *Why?*


3. ### Reproducibility: *Where?*


4.  ### Reproducibllity: *How?*

???

Now for the gist of the talk, how do we apply these principles and which tools do we use to make the whole workflow reproducible, from the data gathering, to its analysis and finally its presentation.

--

* *Principles*
    
* *Tools*

???

So within the how we will discuss the principles which underpin reproducibility in practice such as open data, open research and transparency, and also some of the tools that make this process easier to manage and help you setup a reproducible workflow. 

Again this refers to all the steps in the research process from data gathering, analysis to presentation and disemmenation. I will discuss in particular R and Rstudio, git and make and knitr. 

I should add here that I have previously given a version of this talk that was called more along the lines of reproducible research with R and Rstudio. I have substantially rewritten it so the focus in a lot less on the tools, but they remeain an important part of it, since it would just be too abstract otherwise. 

I have also not gone in the other direction of broadening the focus to include all possible alternative tools. This is partly because I don't know them well from personal experience, and partly because I truly think R and Rstudio are one of the strongest reproducible research environments available. But at least if you are a python user you are in good hands and can set up a workflow that is equivalent. 

But even if you are not an R user, and never intend to become one, I think you should still get something useful out of it, perhaps seeing an implementation of one of the principles might spur you to find an analagous solution that applies better to your setup. 


  
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

???
OK, so first of all I want to make it clear what I mean by reproducibility. In particular in relation to replicability or replication of research. The thing is that both these terms are very often used interchangeably and that can sometimes lead to confusion. So here's the way I will use them here. 
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
.right[ (Jasny, Chin, Chong, and Vignieri, 2011)]

???
We generally understand replication and reproducibility to be the golden standard of scientific research. Here's a definition I like this is for replication: "The confirmation of results and conclusions from one study obtained independently in another". This is from the editorial to a special issue of Science dedicated to "Data replicability and reproducibility". 
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
 .right[ (Jasny, Chin, Chong, et al., 2011)]

* Reproducibility: *"[T]he independent verification of prior findings"*
.right[(Santer, Wigley, and Taylor, 2011)]

???
Here's another good one from the same issue, which is even more condensed: "[T]he independent verification of prior findings", although they are talking about reproducibility here. So like I said, even within the specialists dealing with this topic the terms seem to get mixed and interpreterd synonymously quite often.

---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
.right[ (Jasny, Chin, Chong, et al., 2011)]

* Reproducibility: *"[T]he independent verification of prior findings"*
.right[(Santer, Wigley, and Taylor, 2011)]

**Levels of Replication :**

???

So when I talk about replication I mean it in the broad sense relating to findings and conclusions. But within that I think we can speak of several layers or levels of replication.
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
 .right[ (Jasny, Chin, Chong, et al., 2011)]

* Reproducibility: *"[T]he independent verification of prior findings"*
 .right[(Santer, Wigley, and Taylor, 2011)]

**Levels of Replication :**

1. Re-ask the question

???

The first level is to reask the question from scratch and attempt to confirm a finding or conclusion with a whole new experiment.
--

2. Re-do the experiment

???

The second is to redo an experiment - trying to follow the original methods as closely as possible.
--

3. Re-analyse the data

???

The third level is to reanalyse the data from an experiment - using different methods, models what have you.
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
.right[ (Jasny, Chin, Chong, et al., 2011)] 

* Reproducibility: *"[T]he independent verification of prior findings"*
.right[(Santer, Wigley, and Taylor, 2011)] 

**Levels of Replication :**

1. Re-ask the question

2. Re-do the experiment

3. Re-analyse the data

4. **Reproduce** the results


???

And finally, the strictly narrow sense of reproducibility is reproduce an analysis - using the exact same data, same methods and techniques.

And it's essentially only the last type of replication that I'm talking about here. Sometimes these are also called **pure replications** or **close replications**, but whatever you want to call them, that is what I'm talking about.

Of course some of the principles of reproducibility apply to the broader levels as well, data has to be openly available if anyone is to reanalyse it with their own methods. And experimental methodology must also be documented transparently if anyone is going to replicate your experiment.

But the gist of reproducibility as understood here is ensuring that your results are reproducible using the same data and methods you used.

---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
.right[ (Jasny, Chin, Chong, et al., 2011)]

* Reproducibility: *"[T]he independent verification of prior findings"*
 .right[(Santer, Wigley, and Taylor, 2011)]

**Levels of Replication :**

1. Re-ask the question

2. Re-do the experiment

3. Re-analyse the data

4. **Reproduce** the results

Reproducible workflow: gathering -&gt; analysis -&gt; presentation

???
So to be more concrete, we want to ensure that our whole research workflow, which starts with the gathering of the data, then the analysis and finally the presentation of the results, are reproducible.

---
*Reproducibility means showing your work: *

--

 .right[(imgur user TVsJeff)]

![Show your work *ad absurdum*](/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford//figures/imgur.jpg)

???
So the sort of reproducibility that I'm talking about is about "showing your work". And yes, that can sometimes be the most annoying thing in the world. As in this example here, where a user on imgur vented his annoyance at his maths teacher who wanted him to show his work by taking it to a really extreme conclusion. I'm not sure if you can see this very well, this starts with "Neurons travel through synapses, finger muscles move to grasp writing utensil" and this whole page is just to describe the work that went into writing his name on the homework. And apparently this is followed by 44 more pages like this one.

So obviously this is a bit of an extreme example, but my point is that honestly yes, ensuring your analysis is really reproducible can be quite annoying, and it can seem pointless and unnecessary.

BUT it is worth the hassle in the end. And it will be less of a hassle if you plan systematically on doing it from the start instead of thinking about it retrospecively and trying to fix it after the fact.

And furthermore that doing truly reproducible data analysis has become easier and easier with a variety of tools not only being available, but being accessible, free and relatively easy to use.
---

# Why should research be reproducible?

???
So why would we want it to be reproducible?

--

`1.` For science

--
* opens claims to scrutiny

???

This is one way of distinguishing science---in the positivist sense anyway---from non-science. This is generally considered the ultimate standard for evaluating scientific claims: whether they can be verified through replication.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

???

As a way to further sumulative scientific knowledge, reproducible research cuts down on duplication of effort by allowing scientists to share data or procedures instead of them having to discover things that have already been done by others on their own.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you.

???
I cannot overstate this enough. Despite the sometimes large upfront investment itno learning the tools and setting up your work process, it is worth it even just for making your life a lot easier in the long run

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you.

* forces you to develop better working habits

???

Better work habits mean you are more effective, better organised, produce higher quality work and fewer errors. You're also avoiding personal effort duplication and building your own knowledge base more effectively.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you.

* forces you to develop better working habits

* makes changes and updates of your work easier

???
most research is an iterative process where you keep going back and rerunning parts of your analysis, not a linear one off. additionally you might want to get back to your research after several years have passed and update it with newly available data. reproducible research makes that so much easier. i'm sure we've all been in the position---i certainly have---of going through an old folder for a project of mine, scratching my head trying to figure out what i was trying to do. if you recognize yourself in that, you definitely need to apply some principles of reproducible research to your workflow.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you.

* forces you to develop better working habits

* makes changes and updates of your work easier

* increases your research impact

???
How? Well your research will be more useful to more researchers than if it were not reproducible. It contains more information, it is more likely to be looked at and lead to other research, which means citations of your work.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you.

* forces you to develop better working habits

* makes changes and updates of your work easier

* increases your research impact

* enables easier teamwork

???
Making your work more accessible to colaborators makes it easier to build knowledge together.

--

.right[(Gandrud, 2016)]

???
This outline is from a wonderful book by Christopher Gandrud on reproducible research with R and Rstudio, which I highly recommend as a great hands on introduction to the topic by the way.

---

# Reproducibility in the Humanities?


???

So, I'm a social scientist. A demographer to be more precise, but anyway, the point is I don't really know much about the humanities. And yet I'm fine with telling you about reproducibility and why it's important, without having to know much about the field.

--

* reproducibility is not limited to any branch of science

???

The thing is reproducibility isn't endemic or native to social sciences either. It is an ideal for all research, all scientific endeavour to strive towards. It distinguishes science from non science by opening claims to scrutiny.

As a guiding principle the idea of reproducible research has its stronghold in quantitative research. Research which uses numbers and statistical techniques to obtain results.

--

* but focus is nevertheless on computational research

???

Many of the tools and techniques I will discuss today actually stem explicitly from the fields of computational research: they are tools that programmers and developers use themself. Their efficiency, especially when it comes to colaborative work, depends on being transparent, being able to show their work, being able to systematic about their workflow.

And these tools have been appropriated by quantitative scientists, including in the social sciences and increasingly so in the humanities. The methods of analysis are changing in the humanities as well, with more and more researchers using quantitative analysis in their studies. In this talk I will mainly focus on this sort of reproducibility: of the workflow for computational research including the gathering, analysis and presentation of results.

In this respect the humanities are no different from the social sciences or from the natural sciences. The same principles apply and for the most part the same tools. I will however as an aside say a bit about reproducibility in non-computational research, so in qualitative research.

--

* what about qualitative research?

???

Without getting into the philosophy of science too much, it is of course clear that both the sort of reproducibility and replicability that we defined earlier, are based on a positivist view of science.

Qualitative research does not necessarily follow that, but might be based on an interpretative research method where by definition the conditions of the study are impossible to recreate.

However this doesn't mean we have to revert to the sort of solipsism where only the author understands how the findings were achieved.

--

* Transparency

???

Instead we should use the transparency of the research process to judge the rigour of qualitative research.

--

    1. Production transparency

    2. Data transparency

    3. Analytic transparency


.right[(Moravcsik, 2014)]
???

Following Moravscik we can speak of three dimensions of transparency:

Production transparecny gives information on how the data was produced, gathered and collected.

Data transparency affords readers access to the data used to support the findings.

Analytic transparency gives readers detailed information on the methods of analysisused to interpret the data.

Now although these three types of transparency do not afford the full reproducibility in the narrow sense we discuss above, they do provide enough information to fairly evaluate a study by.

So while this information might not make the research reproducible in the narrow sense of the word, it is the necessary requirement for reproducibility of all reseach.

--

* *cf.* [Annotation for Transparent Inquiry](https://qdr.syr.edu/ati)

???
Furthermore many of the principles that I want to discuss now, still apply to qualitative research, although perhaps not all of the tools. With regard to tools though I would suggest you look at the Annotation for Transparent Inquiry which is an approach to presenting qualitative research in a more transparent way by allowing rich annotations and linking of sources and citations.

---

# Why Tools matter

???

Before we move to discuss the tools for resproducible research, I want to give a quick example of why tools are so important. And I'll try to not make this a militant anti-Excel screed, but instead a thinly veiled anti-Excel screed.


SO I wanted to mention a rather famous story in the history of reproducible research, this one is from the field of macroeconomics. So Carmen Reinhart and Kenneth Rogoff wrote a rather famous paper in 2010 seemingly proving that high levels of GDP suppresses economic growth in countries. Now this was just after the economic crisis and this paper became quite popular as a means of arguing for austerity measures.
--

&lt;div class="figure"&gt;
&lt;img src="/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford//figures/rrexcel.png" alt="Encouraging errors" height="70%" /&gt;
&lt;p class="caption"&gt;Encouraging errors&lt;/p&gt;
&lt;/div&gt;
???

They had not made their data or their analysis freely available, but an american grad student Thomas Herndon was persistent enough in emailing them, that Carmen finally did send him the spreadsheet they used.

And when he looked at it, he found this, the average calculated for this column here, for countries with a debt ratio of 90 percent or more - the average was not calculated for the whole column, but missed five cells at the bottom there.

Which is what happens in Excel. I'm sure you've all done it, dragged across some cells without much control or precision. I know I've definitely done it.

In fact this error - although it got most of the press - didn't actually change the results dramatically, but Herdon was able, once he got hold of the spreadsheet, to also understand how the analysis was conducted and he and many other people agreed that the method of weighting the data was very unorthodox.

Oh, also there was the story recently about the genetic analyses being wrong because some gene names were misinterpreted by Excel as dates. SO there are gene names such as SEPT2, which got routinely transformed into second of september.



---


# Why Tools matter

&lt;div class="figure"&gt;
&lt;img src="/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford//figures/rrexcel.png" alt="Encouraging errors" height="70%" /&gt;
&lt;p class="caption"&gt;Encouraging errors&lt;/p&gt;
&lt;/div&gt;

???

Because effectively they averaged the growth rates regardless of how long they lasted. so the New Zealand growthrate here, -7.9 only lasted a year, but has the same weight as Greece's for example, which I don't know how long it lasted, but I'm pretty sure their debt ratio has been pretty bad for a long time.

Anyway, I just thought I'd mention this story as one about the importance of being transparent and open with your data and analysis, and partly as a cautionary tale about using Excel. Or I should say spreadsheet programmes in general.

Look, I know for a lot of you Excel in a super comfortable go-to programme that gets the job done. And I do think it has its place. But I would very strongly advise against using it in any of your research. But I won't argue too hard against using spreadsheets for prototyping.

---

# Principles and Tools for Reproducible Research

.left-column[
  * *Scripting* your analysis
]
.right-column[
  * `R` (but also e.g. python..)

]

???
 One of the most important principles that underpuns analytic transparency is Scripting.

 s
---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..
]

???
Having your data openly available to other researchers, both ensuring the data sources are adequately described, but also the datasets produced by your analysis made available in stable format.

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*


* *Version Control*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub
]

???
Next we have version control. Version control is a management system that is used to keep track of changes in your files. It is particularly important if your research process is not completely linear. And we know it usually isn't. Version control also provides some element of backup and cloud storage, but this isn't its primary purpose.

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*

* *Version Control*

* *Modularity* and *segmentation*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make
]

???
The next principle is to organise your file structure and your whole workfolw into modular files and describing how the files are connected together in a systematic way. GNU Make in particular is a valuable tool that allows you to execute your whole analysis at once, while keeping track of changes to individual files.

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*

* *Version Control*

* *Modularity* and *segmentation*

* *Literate programming*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU make

* `knitr` &amp; markup languages
]

???
With literate programming we are already in the presentation/publication/dissemenation side of reproducible research, the idea here being that you can combine your text, the formatting and your analytic code in one human readable document that produces your desired output.

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*

* *Version Control*

* *Modularity* and *segmentation*

* *Literate programming*

* *Consistency* and *documentation*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make

* `knitr` &amp; markup languages

* style guides, analysis journals, comments, `packrat`
]

???

Finally the more generic principles of consistency and documentation, don't really have any specific tools, but are usually proscribed to some extent by coding style guides. By consistency I mean e.g. consistent file and variable naming conventions.. And by documentation everything from code comments, variable descriptions and metadata, as well as workprocess documentation in particular citing the software used. `packrat` is additionally an R package that allows you to create a portable and isolated project that cannot be "broken" by future updates or changes.


---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis

* *Data Transparency* or *Open Data*

* *Version Control*

* *Modularity* and *segmentation*

* *Literate programming*

* *Consistency* and *documentation*

* *Convenience*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make

* `knitr` &amp; markup languages

* style guides, analysis journals, comments, `packrat`

* **RStudio**
]

???

I kind of made this last one up, convenience as a principle of Reproducible research.. it obviously isn't a principle or reproducibility, but it is a principle of mine.. The reason I added it here is because of RStudio. Namely RStudio is what is technically called an integrated development environment, for using R, but it now includes capabilities for integrating all the tools mentioned above and in many cases making them a lot simpler to use than they would have been otherwise.

---

# Scripting languages---R

* What did you do?
???

Scripting your analysis is one of the most basic principles of producing reproducible research. Scripting means you are able to answer the most importat question for reproducibility: What did you do?

Remebering what you did, what steps you took in your analysis is a huge tripping point when it comes to retracing your steps. And if you used any sort of point and click steps, it becomes nearly impossible.


---

# Scripting languages---R

* What did you do?

    * Download data

???
Maybe you downloaded the data from somewhere? Did you just click on a link and save it to your Downloads folder? That's a surefire way of loosing track of where you got your data from and which dataset you actually used.


---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

???

Maybe you looked at the data and did some quick edits right off the bat? Removed a header row? Changed a variable name? Changed a missing value to a zero? That's also not reproducible. Unless you scrupulously documented every move you made with your mouse and every key you typed with your keyboard, it will be impossible for anyone coming after you, including yourself, to repeat what you have done.

---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

    * Data analysis


???
After the data cleaning and editing you start your analysis. If you are not using a scripting language you are probably running your analysis using menus, checking tick boxes and selecting subroutines from drop down menus. Or maybe it means copy/pasting columns or writing formulas into cells by dragging your mouse to select data ranges..

The order in which you did these things is also important, and you really have no reasonable way of recording it.

---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

    * Data analysis

    * Data visualisation

???
Finally you might have visualised your data in some way, to produce a chart, or a map for example. In most visualisation software this is a process that is exclusively done in the graphical user interface. You clicked on chart elements to highlight them and change their colours, the widths of lines, etc. Have you ever spent a really long time trying to get your chart to be just perfect, only to then realise you had to do it all again for a new set of data, and getting frustrated when the colours don't match, or you cannot remember the exact styles and formatting you used the first time around?
---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

    * Data analysis

    * Data visualisation

* Scripting:

    * Makes your life easier during your analysis


???

This is where scripting comes in. Scripting languages, such as R, python or Perl, allow you to explicitly write the code for each step of your data gathering process, data cleaning and editing, your analysis and the visualisation.

You can see how this might make your life a whole lot easier. It means the whole process is immediately repeatable, even by you. This is one of the most immediate benefits of moving to a scripted workflow, we've all been there where you have to repeat the same steps several times as you iterate through your analysis and tweak it.

---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

    * Data analysis

    * Data visualisation

* Scripting:

    * Makes your life easier during your analysis

    * Makes everyone elses life easier after your analysis

???

If you use a scripting language you avoid this issue completely. And of course the benefits do not only accrue to you, you have also recorded for posterity your work. For the reviewers of your work, for your readers, for anyone, including yourself, who might want to build on your work.


---

# Scripting languages---R

* What did you do?

    * Download data

    * Data editing

    * Data analysis

    * Data visualisation

* Scripting:

    * Makes your life easier during your analysis

    * Makes everyone elses life easier after your analysis

* Why R?

???

Well, I'm not here to sell R to you, but I will say that it fulfils the scripting criteria very well. It is an increasingly popular data analytics language, and is becoming more user friendly and more powerful every year. It is also open source, which means you are not tied to any commercial licences, and has a wonderful community of users. But if you are e.g. a pyhton user then you really have no reason to switch.

A huge strength of these two languages are their broader development environments which allow easy integration of all of the other tools for reproducibility we will discuss today. With R this comes in the form of RStudio, which I will discuss towards the end.

But there are also second best solutions that should not be scoffed at. If you are for example an SPSS user, there is a relatively straightforward way to start scripting your work without even learning a new programming language. SPSS lets you copy the sytnax code for any command you are about to execute and place it instead into a script file, and run it from there. So if any of you have been trained in SPSS and feel relearning a whole new statistical software might not be worth your while, you can get pretty decent scripting from SPSS, although I don't think you'll do that well on some of the other topics we will discuss. But that's still better than nothing!




---

# Open Data---figshare and other repositories



---

# Version control---git and GitHub

&lt;div class="figure"&gt;
&lt;img src="/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford/figures/final-doc.png" alt="Piled higher and Deeper" width="300px" /&gt;
&lt;p class="caption"&gt;Piled higher and Deeper&lt;/p&gt;
&lt;/div&gt;

???

So first of all let me explain what verison control means:
when you make what is called a commit, you make a snapshot of your project. It is not just saving a single file, but the state of all of your file at a certain point in time.

Every time you make a new commit, git keeps track of the changes that you have made. It is very smart that way in that it doesn't save a whole copy of your project, but only the changes, that way it is very light on resources.

I'm sure we all have folders full of very creatively named files like this.. This quickly gets unwieldly, and is of course very inefficient, since you are duplicating your work in storage every time. Additionally the file names are not very informative anyway. Final_final_now_really final....

Version control therefore allows you to revert back to previous versions of your project, but it also helps you keep track of several paralel versions for example if you are collaborating with someone else. Then you might both be working on parts of the project, and version control ensures that you are able to merge these branches, as they are called, back together.

---

# Version control---git and GitHub

**git**: *a distributed version control system for software development*


???
So git is a distributed version control system for software development. There are other versioning systems, I think git is just one of the most popular ones.

It will make more sense if I tell you it was developed by software developers, coders if you like, for software developers.

The idea is that when you are writing software, stuff works to a point, and then you mess it up somehow and it can be tricky to get back to the previous state, when it was still working.

And to complicate matters further, sometimes you are writing code with someone else, and they add something in their version, and you've added something to yours, and now you want to have both new features work in a new merged version, and you don't want to just be copy pasting stufff across, especially if it will break the code, and then you will have trouble again getting back to the working version from before.

---

# Version control---git and GitHub

**git**: *a distributed version control system for software development*

* git has a learning curve.

???
So like I said, this was written by developers for themselves and git is notorious for being pretty difficult to get a grip of.  You have to enter commands into the terminal, and because it is a super powerfull tool there are many options for any eventuality a serious software developer might be faced with..

--

* but is integrated into Rstudio

???
Luckily for us, git is fully integrated into RStudio, which means actually it is a lot a lot easier to use. Luckily for me anyway. When I first tried using git on my own i quickly got overwhelmed by all the terminology.. I ended up giving up, it was too inpenetrable for me. It was only once it was integrated into RStudio that I started using it again. But now I feel comfortable enough to use it from the command line as well.


Integration in RStudio means you do not have to memorise any shell commands, you just need to add a simple new routine to your workflow, all from the comfort of RStudio.

---

# Version control---git and GitHub

**git**: *a distributed version control system for software development*

* git has a learning curve.

* but is integrated into Rstudio

* So what is GitHub?

???

OK, so I should probably make this distinction clear: git is the name of the software.(and there are others as well, but I'll venture to guess git is the most widely used.

GitHub is the online repository hosting service. Again, there are others such as BitBucket or GitLab, but Git Hub seems to be by far the most popular. The thing is you can totally just use git only on your own machine. That simply means you only have a local repository where your project is stored, and all the version control. But by storing your repository on GitHub, you get a bunch of additional benefits which I'll talk about in a second.


---

# Version control---git and GitHub

Why is version control imortant for reproducibility?

--

* Retracing your steps

???
So why is this relevant for reproducible research. Three reasons: one is for your own sake. So using version control means you can always revert back to any point in the process of your analysis.

--

* Collaboration

???
Second reason is collaboration. Using version control makes it easier to have multiple people working on the same files, same datasets, same coding scripts, same manuscripts while ensuring everyone's contribution is properly recorded, the project files remain consistent and by providing a central platform for all of this to happen, including discussions of issues that might arise.

---

# Version control---git and GitHub

Why is version control imortant for reproducibility?

* Retracing your steps

* Collaboration

* Dissemenation

???
Finally dissemenation. even if you are not collaborating on a project, a github repository is an great tool for dissemination of your work. Simply link to your public repository. This presentation I'm showing you now? I used version control while writing it. I'll share the link with you at the end of the presentation. You will be able to see the final presentation, but if you are interested you can also look through its history and how I worked on it by moving through the commits back in time.

Beign able to share your project's repository is going way beyond an appendix or supplement to your paper that some journals now allow you to publish on line. This means others are able to access your data, your code, and a bunch of other stuff that didn't make it into the final paper.

Did you have to limit yourself to one full colour plot in your paper? Well all the other 50 of them and the code for how to recreate them can be stored in a github repository.

---

# Version control---git and GitHub

Why is version control imortant for reproducibility?

* Retracing your steps

* Collaboration

* Dissemenation

* Authenticated history

???
Note also that git means you have an authenticated history of your project. I'm not sure if it has ever been used this way, but a git version history proves what you did and when, if anyone were to steal your work, this is one way of proving you did it first.

--

* (Backup)

???
Additionally - although this should not be your prime motivation - but github is also a form of backup. The last working version that you committed is stored in the cloud, so there's always that. But you should anyway have dedicated backup solutions in place. GitHub I think allows a maximum of 1GB storage, and single files cannot exceed 100MB. But like I said, it is an extra backup.

---

# Version control---git and GitHub

Why is version control imortant for reproducibility?

* Retracing your steps

* Collaboration

* Dissemenation

* Authenticated history

* (Backup)

* like facebook for coders

???
OK, so this brings me to the final point on my github list and that is calling it facebook for coders. I mean that is maybe a bit of a stretch, but there are some similarities. For one, it is very often possible to find individual users - if they have used a suitably recognizable username.

Which means that like on facebook you can stalk them. Although you can keep your repositories private on GitHub as a paid service, most people prefer to work in open repos. Actually, if you are affiliated with an academic institution you can get five free private repositories. Which is what I would encourage you to do as well. Don't worry if your project is only half finished, just put that in the readme file and anyone that comes across it will know not to expect the finished item.

And this means that you can search for authors you've read and perhaps find the full code for a project they've just published a paper on. You can then make a copy of their repository, what is called cloning a repo, and play with it yourself, perhaps building upon it, and perhaps a collaboration might ensue. I've done that myself.

There is also a more sinister aspect to this - as you would expect with something I'm comparing with gitgub, but that is the fact that these public repositories are all searcheable, including all the code in them. So this is a good one, BEGIN RSA PRIVATE KEY extension:key, and let's see the ones that were most recently indexed. And there you have it. Now I wouldn't know what to do with these, but there have been examples of abuse, for example Uber apparently had the private key for a database of their drivers up here and that got hacked.


---

# GNU Make

*Explicitly tie your files together*


---

# Ensuring *Consistency* and *documentation*

1. style guides

---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments


---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments

4. software citations


---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments

4. software citations

5. session information -- `pacckrat`

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.

???

One way or another it is highly likely that software played an important role in you producing your research. This contribution should be acknowledged. Now this doesn't mean you have to make sure everyone knows you used Microsoft Word to write your paper, a good rule of thumb is to ask yourself if the software contributed critically to your research and/or if it provided something novel. This is the recommendation of the British Software sustainability insititute. If this feels a bit ambiguous, I would suggest you err on the side of citing rather than not.

For example you might think it silly to acknowledge that you used Microsoft Excel to do your analysis, but that is only if you thing it the software is faultless and your analysis would have produced the exact same results in any spreadhseet programme. But that may not be true. Excel--not to pick on any specific programme--but it has a bunch of quirks that are specific to it, you might even call them bugs. And they could very well affect your analysis, which counts as having a critical contribtution.

For example Excel treats 1900 as a leap year. This is due to some historical reasons, to make it compatible with IBM's Lotus-1-2-3 if you're old enough to remember it. Anyway, Microsoft aknowledges that this means "The WEEKDAY function returns incorrect values for dates before March 1, 1900. Because most users do not use dates before March 1, 1900, this problem is rare."

But more generally you should cite any software that impacts upon the results, includes numerical modelling or simulations, any algorithmic evaluations or research using software that does some form of automated analysis e.g. image analysis or optical character recognition.

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.
  * sometimes the licence requires you cite it

???

Sometimes the decision is made for you, because the licence for the software, that you have of course carefully read, so you know this, but sometimes the licence explicitly requires you cite it.

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.
  * sometimes the licence requires you cite it

* How should it be cited?

  * find your citation style's examples

???

All the main citation styles have examples of how to cite software. For example MLA suggests:


And you should be able to look up examples for your prefered style. Whatever you choose though, make sure you include the version of the software. The idea is for you to be transparent about how your results were produced and allow someone else to replicate them, and software versions could play a role here.

Additionally most software developers will have a suggested citation format.


---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.
  * sometimes the licence requires you cite it

* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!

???
Sometimes it seems more straightforward to cite the associated paper for a programme instead of the software itself. It fits the format we're used to, I guess it's always nicer to cite a person than an institution or company. You should avoid doing that. Always cite the software explicitly. If the associated paper was substatnively useful in your analysis, then by all means cite it as well, additionally, but not instead.

Several reasons for this: not all software has associated journal articles. Also an article is not specific to the version you were using, so does not serve to enhance reproducibility in that way.


---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.
  * sometimes the licence requires you cite it

* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!
  * if there is a DOI, use it!

???

If the software you are using has a DOI, then use it! Use it even at the expense of a url. Because DOIs are perseistent and urls are not. If you cite the DOI in its url form with the resolver service url as its prefix, for example like this http://dx.doi.org/NNNN,then it will work like a link and lead to the source, even if it has moved in the meantime.


---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution.
  * sometimes the licence requires you cite it

* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!
  * if there is a DOI, use it!

* Where should you cite it?

  * if cannot cite in references then footnotes are second best option
  * or methods section or appendix or supplementary materials, as long as it is there!

 .right[(Jackson, 2012)]


???

Publishers and reviewers might take issue with the citing of software, however that sort of attitide is hopefully becoming a thing of the past. If you were to receive pushback the alternative is always to include the citation information in a footnote or otherwise stated in the methods section. From the point of view of reproducibility, the important thing is the information is accessible to the reader. However from the point of view of the authors, especially if they have mandated citation in the licence, that may be too little.


If there is no DOI, use urls, even though MLA recommends against it (becuase of breakage). It is still better than nothing.


---

# Literate programming---`knitr` and markup languages

.center[*“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”*]

.right[Donald Knuth (1984)]

???
So the next important concept in reproducible research  I want to talk about, and one that is super easy with R and RStudio, is literate programming. So this concept was devised by Donald Knuth, who is the legend behind the TeX typesetting system amongst other things.

Anyway, the basic idea is that your code should be human readable. Now of course strictly speaking your code will always be human readable, but the question is how frustrated do you want that human to be. Because of course technically any machine readable code can be read by a human, and they can eventually figure out what the code does.

In fact many of the topics we just covered under documentation and consistency also contribute to literate programming, especially documentation and commenting. Literate programming is in a way an overarching goal to make your code human readable and comprehensive.

---

# Literate programming---`knitr` and markup languages

.center[*“Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”*]

.right[Donald Knuth (1984)]

* Human readable and comprehensive:

???

So what do we mean by human readable and coprehensive?
--
    * Pure code: How and what, but not why?
    * Text only: Why and what, but not how?

???

Well pure code can let the reader know what was done and how, but not why? Similarly reading a analysis report also allows a human to understand what the analysis did, and why but it does not fully explain HOW.

So what we want ideally is to be able to easily understand the what, how and why of the analysis, and there are a couple of things to keep in mind here. We've already mentioned scripting your work, as well as commenting it liberally and documenting your work.

---

# Literate programming---`knitr` and markup languages

**knitting** together:
  * code
  * text
  * formatting
  * outputs (tables and charts)

???
But really the pinacle of Literate programming is *knitting*

The idea is that you knit together the code and the explanatory text, going beyond just commenting, which is just code and text, but also includes formatting, plots and tables.

---

# Literate programming---`knitr` and markup languages

**knitting** together:
  * code
  * text
  * formatting
  * outputs (tables and charts)

**Markup languages**:
  * TeX and LaTeX
  * html
  * markdown, Rmarkdown

???
In order to be able to do that you need to be able to combine the code with the formatted text in a single text file, which means you cannot use for example Word to format your text. 

Instead you have to use a markup language. Which is basically a system for annotating text with formatting. LaTeX is a well known and very powerful markup language, it is in fact the one that Donald Knuth invented so to speak, html is also a markup language. A more user friendly markup language is markdown and the R flavour of markdown which is Rmarkdown. 


--- 
# Literate programming---`knitr` and markup languages

???

so Markdown is a markup language, but the whole idea of it is to make it really easy to use, and really easy to read - so instead of latex html which for example has loads of tags and reading it directly is pretty cumbersome, markdown is actually very unobtrusive.
--

&lt;img src="/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford/figures/html.png" width="400px" /&gt;

???

So just to give you a quick flavour of how it works, here we have on the left hand side some text formatted using Markdown, we see we can have different level headings, the usuall formatting italic, bold etcetera, bulleted lists, enumerated lists, tables. And here on the right is how this would be viewed in your browser. So maybe it doesn't look supper pretty - you will just have to learn how to use Latex if you want that - but it definitely does the job and is incredibly simple to learn. 

knitting your markup language of choice and your code together allows you to produce an output document, be it a pdf report, a presentation, even a word document directly from one human readable text file. And that is the essence of literate programming. 


And you can appreciate how this is relevant from a reproducibility point of view. If your whole analysis is knitted together like that it means if something in the input data changes, you can automatically update the whole report by recompiling the report from this one file. 


 &lt;!-- (Jackson, 2012) --&gt;


 &lt;!-- (Hong, Crick, Gent, Kotthoff, and Takeda, 2015) --&gt;
---

# References

[1] C. Gandrud. _Reproducible research with R and R studio_.
Chapman and Hall/CRC, 2016.

[2] N. P. C. Hong, T. Crick, I. P. Gent, et al. "Top tips to make
your research irreproducible". In: _arXiv preprint
arXiv:1504.00062_ (2015).

[3] M. Jackson. _How to cite and describe software_. &lt;URL:
https://software.ac.uk/how-cite-software&gt;. [Online; accessed
22.7.2019]. 2012.

[4] B. R. Jasny, G. Chin, L. Chong, et al. "Again, and Again, and
Again..." In: _Science_ 334.6060 (2011), pp. 1225-1225. DOI:
[10.1126/science.334.6060.1225](https://doi.org/10.1126%2Fscience.334.6060.1225).

[5] A. Moravcsik. "Transparency: The Revolution in Qualitative
Research". In: _PS: Political Science &amp; Politics_ 47.1 (2014), p.
48–53. DOI:
[10.1017/S1049096513001789](https://doi.org/10.1017%2FS1049096513001789).

[6] B. D. Santer, T. M. L. Wigley, and K. E. Taylor. "The
Reproducibility of Observational Estimates of Surface and
Atmospheric Temperature Change". In: _Science_ 334.6060 (2011),
pp. 1232-1233. ISSN: 0036-8075. DOI:
[10.1126/science.1216273](https://doi.org/10.1126%2Fscience.1216273).

---

# Thank you!

&lt;img src="/home/mz/Dropbox/XtraWork/teaching/2019-07-rep-res-humanities-oxford/figures/qr-code.png" width="400px" /&gt;

GitHub repository for this presentation:

[GitHub repository for this presentation: https://github.com/majazaloznik/2019-07-rep-res-humanities-oxford"](https://github.com/majazaloznik/2019-07-rep-res-humanities-oxford")
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
