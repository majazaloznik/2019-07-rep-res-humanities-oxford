---
title: "Reproducible Research"
subtitle: "(in the Humanities)"
author: "Maja Zalo≈ænik"
date: "25 July 2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["my-css.css", default-fonts]
    nature:
      highlightStyle: github
      countIncrementalSlides: false
bibliography: ..\/..\/data\/dictionaries\/lit.bib
---

```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR) 
BibOptions(check.entries = FALSE,
           #bib.style = "alphabetic",
           cite.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib(here::here("data/dictionaries/lit.bib"), check = FALSE)
```

# Outline of this talk

--

* ### Reproducibility: *What?*


???

So first of all we should try to get on the same playing field, I want to briefly talk about what Reproducibility is, what we mean by reproducible research. 
--

* ### Reproducibility: *Why?*

???

Next i will talk about WHY reproducible research is important, why it should be important to you

--

* ### Reproducibility: *Where?*


???

By where I mean "in the humanities", so I'll briefly touch on how we think about reproducibility in computational fields and in non-computational fields, although the focus of the talk is of course the former. 

---
# Outline of this talk

* ### Reproducibility: *What?*


* ### Reproducibility: *Why?*


* ### Reproducibility: *Where?*


* ### Reproducibllity: *How?*

???

Now for the gist of the talk, how do we apply these principles and which tools do we use to make the whole workflow reproducible, from the data gathering, to its analysis and finally its presentation.

--

    + #### *Principles*
    + #### *Tools*

???

So within the how we will discuss the principles which underpin reproducibility in practice such as open data, open research and transparency, and also some of the tools that make this process easier to manage and help you setup a reproducible workflow. 

Again this refers to all the steps in the research process from data gathering, analysis to presentation and disemmenation. I will discuss in particular R and Rstudio, git and make and knitr. 

I should add here that I have previously given a version of this talk that was called more along the lines of reproducible research with R and Rstudio. I have substantially rewritten it so the focus in a lot less on the tools, but they remeain an important part of it, since it would just be too abstract otherwise. 

I have also not gone in the other direction of broadening the focus to include all possible alternative tools. This is partly because I don't know them well from personal experience, and partly because I truly think R and Rstudio are one of the strongest reproducible research environments available. But at least if you are a python user you are in good hands and can set up a workflow that is equivalent. 

But even if you are not an R user, and never intend to become one, I think you should still get something useful out of it, perhaps seeing an implementation of one of the principles might spur you to find an analagous solution that applies better to your setup. 


  
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

???
OK, so first of all I want to make it clear what I mean by reproducibility. In particular in relation to replicability or replication of research. The thing is that both these terms are very often used interchangeably and that can sometimes lead to confusion. So here's the way I will use them here. 
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

???
We generally understand replication and reproducibility to be the golden standard of scientific research. Here's a definition I like this is for replication: "The confirmation of results and conclusions from one study obtained independently in another". This is from the editorial to a special issue of Science dedicated to "Data replicability and reproducibility". 
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

* Reproducibility: *"[T]he independent verification of prior findings"*
<!-- .right[`r Citep(myBib, "Santer1232")`] -->

???
Here's another good one from the same issue, which is even more condensed: "[T]he independent verification of prior findings", although they are talking about reproducibility here. So like I said, even within the specialists dealing with this topic the terms seem to get mixed and interpreterd synonymously quite often. 

---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

* Reproducibility: *"[T]he independent verification of prior findings"*
<!-- .right[`r Citep(myBib, "Santer1232")`] -->
**Levels of Replication :**

???

So when I talk about replication I mean it in the broad sense relating to findings and conclusions. But within that I think we can speak of several layers or levels of replication.
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

* Reproducibility: *"[T]he independent verification of prior findings"*
<!-- .right[`r Citep(myBib, "Santer1232")`] -->
**Levels of Replication :**

1. Re-ask the question 

???

The first level is to reask the question from scratch and attempt to confirm a finding or conclusion with a whole new experiment.


2. Re-do the experiment 

???

The second is to redo an experiment - trying to follow the original methods as closely as possible.
--

3. Re-analyse the data

???

The third level is to reanalyse the data from an experiment - using different methods, models what have you.
---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

* Reproducibility: *"[T]he independent verification of prior findings"*
<!-- .right[`r Citep(myBib, "Santer1232")`] -->
**Levels of Replication :**

1. Re-ask the question 

2. Re-do the experiment 

3. Re-analyse the data

4. **Reproduce** the results


???

And finally, the strictly narrow sense of reproducibility is reproduce an analysis - using the exact same data, same methods and techniques.

And it's essentially only the last type of replication that I'm talking about here. Sometimes these are also called **pure replications** or **close replications**, but whatever you want to call them, that is what I'm talking about. 

Of course some of the principles of reproducibility apply to the broader levels as well, data has to be openly available if anyone is to reanalyse it with their own methods. And experimental methodology must also be documented transparently if anyone is going to replicate your experiment. 

But the gist of reproducibility as understood here is ensuring that your results are reproducible using the same data and methods you used. 

---

# What is Reproducibility?

* Reproducibility vs Replicability or Replication?

* Replication: *"The confirmation of results and conclusions from one study obtained independently in another"*
<!-- .right[ `r Citep(myBib, "Jasny1225")`] -->

* Reproducibility: *"[T]he independent verification of prior findings"*
<!-- .right[`r Citep(myBib, "Santer1232")`] -->
**Levels of Replication :**

1. Re-ask the question 

2. Re-do the experiment 

3. Re-analyse the data

4. **Reproduce** the results

Reproducible workflow: gathering -> analysis -> presentation 

???
So to be more concrete, we want to ensure that our whole research workflow, which starts with the gathering of the data, then the analysis and finally the presentation of the results, are reproducible. 

---
*Reproducibility means showing your work: *

--

 .right[(imgur user TVsJeff)]
 
```{r imgur, echo=FALSE, fig.cap="Show your work *ad absurdum*", fig.height=5}
knitr::include_graphics(here::here("/figures/imgur.jpg"))
```

???
So the sort of reproducibility that I'm talking about is about "showing your work". And yes, that can sometimes be the most annoying thing in the world. As in this example here, where a user on imgur vented his annoyance at his maths teacher who wanted him to show his work by taking it to a really extreme conclusion. I'm not sure if you can see this very well, this starts with "Neurons travel through synapses, finger muscles move to grasp writing utensil" and this whole page is just to describe the work that went into writing his name on the homework. And apparently this is followed by 44 more pages like this one. 

So obviously this is a bit of an extreme example, but my point is that honestly yes, ensuring your analysis is really reproducible can be quite annoying, and it can seem pointless and unnecessary.  

BUT it is worth the hassle in the end. And it will be less of a hassle if you plan systematically on doing it from the start instead of thinking about it retrospecively and trying to fix it after the fact. 

And furthermore that doing truly reproducible data analysis has become easier and easier with a variety of tools not only being available, but being accessible, free and relatively easy to use. 
---

# Why should research be reproducible?

???
So why would we want it to be reproducible?

--

`1.` For science

--
* opens claims to scrutiny

???

This is one way of distinguishing science---in the positivist sense anyway---from non-science. This is generally considered the ultimate standard for evaluating scientific claims: whether they can be verified through replication. 

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

???

As a way to further sumulative scientific knowledge, reproducible research cuts down on duplication of effort by allowing scientists to share data or procedures instead of them having to discover things that have already been done by others on their own. 

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you. 

???
I cannot overstate this enough. Despite the sometimes large upfront investment itno learning the tools and setting up your work process, it is worth it even just for making your life a lot easier in the long run

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you. 

* forces you to develop better working habits

???

Better work habits mean you are more effective, better organised, produce higher quality work and fewer errors. You're also avoiding personal effort duplication and building your own knowledge base more effectively.

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you. 

* forces you to develop better working habits

* makes changes and updates of your work easier

???
most research is an iterative process where you keep going back and rerunning parts of your analysis, not a linear one off. additionally you might want to get back to your research after several years have passed and update it with newly available data. reproducible research makes that so much easier. i'm sure we've all been in the position---i certainly have---of going through an old folder for a project of mine, scratching my head trying to figure out what i was trying to do. if you recognize yourself in that, you definitely need to apply some principles of reproducible research to your workflow. 

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you. 

* forces you to develop better working habits

* makes changes and updates of your work easier

* increases your research impact

???
How? Well your research will be more useful to more researchers than if it were not reproducible. It contains more information, it is more likely to be looked at and lead to other research, which means citations of your work. 

---

# Why should research be reproducible?

`1.` For science

* opens claims to scrutiny

* helps avoid duplication and encourage cumulation of knwoledge

`2.` For you. 

* forces you to develop better working habits

* makes changes and updates of your work easier

* increases your research impact

* enables easier teamwork

???
Making your work more accessible to colaborators makes it easier to build knowledge together. 

--

.right[`r Citep(myBib, "gandrud2016reproducible")`]

???
This outline is from a wonderful book by Christopher Gandrud on reproducible research with R and Rstudio, which I highly recommend as a great hands on introduction to the topic by the way. 

---

# Reproducibility in the Humanities?


???

So, I'm a social scientist. A demographer to be more precise, but anyway, the point is I don't really know much about the humanities. And yet I'm fine with telling you about reproducibility and why it's important, without having to know much about the field. 

--

* reproducibility is not limited to any branch of science

???

The thing is reproducibility isn't endemic or native to social sciences either. It is an ideal for all research, all scientific endeavour to strive towards. It distinguishes science from non science by opening claims to scrutiny. 

As a guiding principle the idea of reproducible research has its stronghold in quantitative research. Research which uses numbers and statistical techniques to obtain results. 

-- 

* but focus is nevertheless on computational research

???

Many of the tools and techniques I will discuss today actually stem explicitly from the fields of computational research: they are tools that programmers and developers use themself. Their efficiency, especially when it comes to colaborative work, depends on being transparent, being able to show their work, being able to systematic about their workflow. 

And these tools have been appropriated by quantitative scientists, including in the social sciences and increasingly so in the humanities. The methods of analysis are changing in the humanities as well, with more and more researchers using quantitative analysis in their studies. In this talk I will mainly focus on this sort of reproducibility: of the workflow for computational research including the gathering, analysis and presentation of results. 

In this respect the humanities are no different from the social sciences or from the natural sciences. The same principles apply and for the most part the same tools. I will however as an aside say a bit about reproducibility in non-computational research, so in qualitative research.

--

* what about qualitative research?

???

Without getting into the philosophy of science too much, it is of course clear that both the sort of reproducibility and replicability that we defined earlier, are based on a positivist view of science. 

Qualitative research does not necessarily follow that, but might be based on an interpretative research method where by definition the conditions of the study are impossible to recreate. 

However this doesn't mean we have to revert to the sort of solipsism where only the author understands how the findings were achieved. 

--

* Transparency

???

Instead we should use the transparency of the research process to judge the rigour of qualitative research. 

--

    1. Production transparency
    
    2. Data transparency
    
    3. Analytic transparency

  
.right[`r Citep(myBib, "moravcsik_2014")`]
???

Following Moravscik we can speak of three dimensions of transparency: 

Production transparecny gives information on how the data was produced, gathered and collected. 

Data transparency affords readers access to the data used to support the findings. 

Analytic transparency gives readers detailed information on the methods of analysisused to interpret the data. 

Now although these three types of transparency do not afford the full reproducibility in the narrow sense we discuss above, they do provide enough information to fairly evaluate a study by.

So while this information might not make the research reproducible in the narrow sense of the word, it is the necessary requirement for reproducibility of all reseach. 

--

* *cf.* [Annotation for Transparent Inquiry](https://qdr.syr.edu/ati)

???
Furthermore many of the principles that I want to discuss now, still apply to qualitative research, although perhaps not all of the tools. With regard to tools though I would suggest you look at the Annotation for Transparent Inquiry which is an approach to presenting qualitative research in a more transparent way by allowing rich annotations and linking of sources and citations. 

---

# Why Tools matter

???

Before we move to discuss the tools for resproducible research, I want to give a quick example of why tools are so important. And I'll try to not make this a militant anti-Excel screed, but instead a thinly veiled anti-Excel screed. 


SO I wanted to mention a rather famous story in the history of reproducible research, this one is from the field of macroeconomics. So Carmen Reinhart and Kenneth Rogoff wrote a rather famous paper in 2010 seemingly proving that high levels of GDP suppresses economic growth in countries. Now this was just after the economic crisis and this paper became quite popular as a means of arguing for austerity measures.
--

```{r rrexcel2, echo=FALSE, fig.cap="Encouraging errors", out.widht = '70%', out.height='70%'}
knitr::include_graphics(here::here("/figures/rrexcel.png"))
```
???

They had not made their data or their analysis freely available, but an american grad student Thomas Herndon was persistent enough in emailing them, that Carmen finally did send him the spreadsheet they used. 

And when he looked at it, he found this, the average calculated for this column here, for countries with a debt ratio of 90 percent or more - the average was not calculated for the whole column, but missed five cells at the bottom there.

Which is what happens in Excel. I'm sure you've all done it, dragged across some cells without much control or precision. I know I've definitely done it. 

In fact this error - although it got most of the press - didn't actually change the results dramatically, but Herdon was able, once he got hold of the spreadsheet, to also understand how the analysis was conducted and he and many other people agreed that the method of weighting the data was very unorthodox. 

Oh, also there was the story recently about the genetic analyses being wrong because some gene names were misinterpreted by Excel as dates. SO there are gene names such as SEPT2, which got routinely transformed into second of september.



---


# Why Tools matter

```{r rrexcel, echo=FALSE, fig.cap="Encouraging errors", out.widht = '70%', out.height='70%'}
knitr::include_graphics(here::here("/figures/rrexcel.png"))
```

???

Because effectively they averaged the growth rates regardless of how long they lasted. so the New Zealand growthrate here, -7.9 only lasted a year, but has the same weight as Greece's for example, which I don't know how long it lasted, but I'm pretty sure their debt ratio has been pretty bad for a long time.

Anyway, I just thought I'd mention this story as one about the importance of being transparent and open with your data and analysis, and partly as a cautionary tale about using Excel. Or I should say spreadsheet programmes in general. 

Look, I know for a lot of you Excel in a super comfortable go-to programme that gets the job done. And I do think it has its place. But I would very strongly advise against using it in any of your research. But I won't argue too hard against using spreadsheets for prototyping. 

---

# Principles and Tools for Reproducible Research

.left-column[
  * *Scripting* your analysis 
]
.right-column[
  * `R` (but also e.g. python..)

]

???
 One of the most important principles that underpuns analytic transparency is Scripting. 
 
 s
---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..
]

???
Having your data openly available to other researchers, both ensuring the data sources are adequately described, but also the datasets produced by your analysis made available in stable format.

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*


* *Version Control* 
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub 
]

???
Next we have version control. Version control is a management system that is used to keep track of changes in your files. It is particularly important if your research process is not completely linear. And we know it usually isn't. Version control also provides some element of backup and cloud storage, but this isn't its primary purpose. 

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*

* *Version Control* 

* *Modularity* and *segmentation*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make
]

???
The next principle is to organise your file structure and your whole workfolw into modular files and describing how the files are connected together in a systematic way. GNU Make in particular is a valuable tool that allows you to execute your whole analysis at once, while keeping track of changes to individual files. 

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*

* *Version Control* 

* *Modularity* and *segmentation*

* *Literate programming*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU make

* `knitr` & markup languages
]

???
With literate programming we are already in the presentation/publication/dissemenation side of reproducible research, the idea here being that you can combine your text, the formatting and your analytic code in one human readable document that produces your desired output. 

---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*

* *Version Control* 

* *Modularity* and *segmentation*

* *Literate programming*

* *Consistency* and *documentation*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make

* `knitr` & markup languages

* style guides, analysis journals, comments, `packrat`
]

???

Finally the more generic principles of consistency and documentation, don't really have any specific tools, but are usually proscribed to some extent by coding style guides. By consistency I mean e.g. consistent file and variable naming conventions.. And by documentation everything from code comments, variable descriptions and metadata, as well as workprocess documentation in particular citing the software used. `packrat` is additionally an R package that allows you to create a portable and isolated project that cannot be "broken" by future updates or changes. 


---

# Principles and Tools for Reproducible Research

.left-column[
* *Scripting* your analysis 

* *Data Transparency* or *Open Data*

* *Version Control* 

* *Modularity* and *segmentation*

* *Literate programming*

* *Consistency* and *documentation*

* *Convenience*
]
.right-column[
* `R` (but also e.g. python..)

* fighsare, institutional repositories..

* git and GitHub

* GNU Make

* `knitr` & markup languages

* style guides, analysis journals, comments, `packrat`

* **RStudio**
]

???

I kind of made this last one up, convenience as a principle of Reproducible research.. it obviously isn't a principle or reproducibility, but it is a principle of mine.. The reason I added it here is because of RStudio. Namely RStudio is what is technically called an integrated development environment, for using R, but it now includes capabilities for integrating all the tools mentioned above and in many cases making them a lot simpler to use than they would have been otherwise. 

---

# R

---

# figshare and other repositories

---

# git and GitHub

```{r git, echo=FALSE,  fig.cap= "https://xkcd.com/1597/", out.width = "300px"}
knitr::include_graphics(here::here("figures/git.png"))
```

---

# GNU Make

---

# `knitr` and markup languages

---

# Ensuring *Consistency* and *documentation*

1. style guides

---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments


---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments

4. software citations


---

# Ensuring *Consistency* and *documentation*

1. style guides

2. analysis journals

3. comments

4. software citations

5. `pacckrat`

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  
???

One way or another it is highly likely that software played an important role in you producing your research. This contribution should be acknowledged. Now this doesn't mean you have to make sure everyone knows you used Microsoft Word to write your paper, a good rule of thumb is to ask yourself if the software contributed critically to your research and/or if it provided something novel. This is the recommendation of the British Software sustainability insititute. If this feels a bit ambiguous, I would suggest you err on the side of citing rather than not. 

For example you might think it silly to acknowledge that you used Microsoft Excel to do your analysis, but that is only if you thing it the software is faultless and your analysis would have produced the exact same results in any spreadhseet programme. But that may not be true. Excel--not to pick on any specific programme--but it has a bunch of quirks that are specific to it, you might even call them bugs. And they could very well affect your analysis, which counts as having a critical contribtution. 

For example Excel treats 1900 as a leap year. This is due to some historical reasons, to make it compatible with IBM's Lotus-1-2-3 if you're old enough to remember it. Anyway, Microsoft aknowledges that this means "The WEEKDAY function returns incorrect values for dates before March 1, 1900. Because most users do not use dates before March 1, 1900, this problem is rare."

But more generally you should cite any software that impacts upon the results, includes numerical modelling or simulations, any algorithmic evaluations or research using software that does some form of automated analysis e.g. image analysis or optical character recognition. 

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  * sometimes the licence requires you cite it
  
???

Sometimes the decision is made for you, because the licence for the software, that you have of course carefully read, so you know this, but sometimes the licence explicitly requires you cite it.  

---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  * sometimes the licence requires you cite it
  
* How should it be cited?

  * find your citation style's examples

???

All the main citation styles have examples of how to cite software. For example MLA suggests: 


And you should be able to look up examples for your prefered style. Whatever you choose though, make sure you include the version of the software. The idea is for you to be transparent about how your results were produced and allow someone else to replicate them, and software versions could play a role here. 

Additionally most software developers will have a suggested citation format. 


---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  * sometimes the licence requires you cite it
  
* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!
  
???
Sometimes it seems more straightforward to cite the associated paper for a programme instead of the software itself. It fits the format we're used to, I guess it's always nicer to cite a person than an institution or company. You should avoid doing that. Always cite the software explicitly. If the associated paper was substatnively useful in your analysis, then by all means cite it as well, additionally, but not instead. 

Several reasons for this: not all software has associated journal articles. Also an article is not specific to the version you were using, so does not serve to enhance reproducibility in that way. 


---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  * sometimes the licence requires you cite it
  
* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!
  * if there is a DOI, use it!
  
???

If the software you are using has a DOI, then use it! Use it even at the expense of a url. Because DOIs are perseistent and urls are not. If you cite the DOI in its url form with the resolver service url as its prefix, for example like this http://dx.doi.org/NNNN,then it will work like a link and lead to the source, even if it has moved in the meantime. 
 
 
---

# How to cite software

* Which software should be cited?

  * Critical and/or novel contribution. 
  * sometimes the licence requires you cite it
  
* How should it be cited?

  * find your citation style's examples
  * do not use cite associated papers instead!
  * if there is a DOI, use it!
  
* Where should you cite it?

  * if cannot cite in references then footnotes are second best option
  * or methods section or appendix or supplementary materials, as long as it is there!
 
 .right[`r Citep(myBib, "jacksonNodate")`]
 

???

Publishers and reviewers might take issue with the citing of software, however that sort of attitide is hopefully becoming a thing of the past. If you were to receive pushback the alternative is always to include the citation information in a footnote or otherwise stated in the methods section. From the point of view of reproducibility, the important thing is the information is accessible to the reader. However from the point of view of the authors, especially if they have mandated citation in the licence, that may be too little. 


If there is no DOI, use urls, even though MLA recommends against it (becuase of breakage). It is still better than nothing.





 <!-- `r Citep(myBib, "jacksonNodate")` -->
 
 
 <!-- `r Citep(myBib, "hong2015top")` -->
---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib, start = 1)
```

---

# Thank you!

```{r qr, echo=FALSE,  out.width = "400px"}
knitr::include_graphics(here::here("figures/qr-code.png"))
```

GitHub repository for this presentation:

[GitHub repository for this presentation: https://github.com/majazaloznik/2019-07-rep-res-humanities-oxford"](https://github.com/majazaloznik/2019-07-rep-res-humanities-oxford")


